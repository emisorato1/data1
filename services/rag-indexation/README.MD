# ğŸš€ Enterprise AI Platform - Metadata Indexing Pipeline

Este servicio es el motor de extracciÃ³n y procesamiento de datos estructurados del ECM (Content Server). Su objetivo es transformar la complejidad de las tablas de OpenText en un contrato de datos listo para la vectorizaciÃ³n (RAG).

> **Nota CrÃ­tica:** Este desarrollo cumple estrictamente con el documento **Enterprise AI Platform â€“ RAG Indexing Service**.

## ğŸ—ï¸ Arquitectura Medallion

Implementamos una segregaciÃ³n de datos en tres capas para garantizar auditabilidad y reproducibilidad (SecciÃ³n 1 de la documentaciÃ³n).

### ğŸ¥‰ Capa 1_Bronze: ExtracciÃ³n Cruda
* **PropÃ³sito:** Es el "espejo" del sistema origen. No aplica lÃ³gica, solo persiste.
* **Componentes Clave:** * `extract_metadata.sql`: Query optimizada que unifica las 3 "islas" de OpenText (`DTreeCore`, `DVersData`, `ProviderData`).
    * **ExtracciÃ³n Incremental:** El sistema detecta automÃ¡ticamente la fecha del Ãºltimo documento procesado para evitar duplicidad de trabajo.
* **GarantÃ­as:** Checksum SHA-256 del payload completo para auditorÃ­a (SecciÃ³n 5).

### ğŸ¥ˆ Capa 2_Silver: Limpieza y Seguridad
* **PropÃ³sito:** Transformar datos SQL en informaciÃ³n de negocio indexable.
* **Reglas de Negocio:**
    1. **Filtrado:** Solo se procesan MimeTypes permitidos (PDF, DOCX, TXT).
    2. **Seguridad HÃ­brida:** ConversiÃ³n de la matriz de permisos ACL en **Security Tokens** con formato `Tipo:ID:Nivel` (SecciÃ³n 5.6).
    3. **EstandarizaciÃ³n:** Fechas convertidas a **ISO 8601**.

### ğŸ¥‡ Capa 3_Gold: Contrato de Datos Final
* **PropÃ³sito:** Generar el payload inmutable para el ensamble con el Pipeline de Archivos.
* **GarantÃ­as:**
    * **Identidad:** IDs normalizados con formato `DOC-CS-{id}-v{version}`.
    * **Integridad:** `integrity_hash` individual por registro. Si el contenido o la seguridad cambian, el hash cambia.

---

## ğŸ“ˆ Trazabilidad y Linaje (Data Lineage)

Siguiendo la **SecciÃ³n 4**, cada ejecuciÃ³n es rastreable mediante un `run_id` Ãºnico.

| Artefacto Generado | UbicaciÃ³n | DescripciÃ³n |
| --- | --- | --- |
| **Bronze JSON** | `data/1_bronze/BRZ_{id}.json` | Datos SQL crudos + Metadata de extracciÃ³n. |
| **Silver JSON** | `data/2_silver/SLV_{id}.json` | Documentos con seguridad resuelta y normalizados. |
| **Gold JSON** | `data/3_gold/GLD_{id}.json` | **Contrato Final**. Punto de entrada para la Vector DB. |

---

## ğŸ› ï¸ Estructura del MÃ³dulo

DiseÃ±ado para ser integrado en el Monorepo corporativo:

```text
metadata-pipelines/
â”œâ”€â”€ config/             # Variables .env (SERVER, DB_DRIVER, etc.)
â”œâ”€â”€ data/               # "Data Lake" Local segregado
â”‚   â”œâ”€â”€ 1_bronze/
â”‚   â”œâ”€â”€ 2_silver/
â”‚   â””â”€â”€ 3_gold/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ common/         # DatabaseManager, Utils y Logger
â”‚   â”œâ”€â”€ pipeline/       # LÃ³gica de las 3 capas Medallion
â”‚   â””â”€â”€ main.py         # Orquestador con lÃ³gica incremental
â””â”€â”€ pyproject.toml      # ConfiguraciÃ³n de dependencias con uv

```

---

## ğŸ¤ Instrucciones para el Pipeline de DATA

1. Utiliza el archivo generado en **`data/gold/GLD_{run_id}.json`**.
2. El campo `source_metadata.efs_path` contiene el nombre real del archivo en el Archive Center (ej: `103744.dat`).
3. El campo `document_id` es la clave primaria que debes usar para asociar tus vectores con nuestra metadata.

--- 

## ğŸ› ï¸ ConfiguraciÃ³n del Entorno (Developer Setup)

Este proyecto utiliza **uv** para una gestiÃ³n de dependencias ultra-rÃ¡pida y determinÃ­stica.

1. **Instalar uv** (si no lo tienes):
   `curl -LsSf https://astral.sh/uv/install.sh | sh`

2. **Instalar dependencias y crear venv**:
   `uv sync`

3. **Ejecutar el Pipeline**:
   `uv run src/main.py`
---

*DocumentaciÃ³n tÃ©cnica generada bajo estÃ¡ndares de alta criticidad.*

